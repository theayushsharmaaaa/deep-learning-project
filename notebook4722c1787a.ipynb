{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":584717,"sourceType":"datasetVersion","datasetId":284040},{"sourceId":7876159,"sourceType":"datasetVersion","datasetId":1640734}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install dependencies","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install /kaggle/input/torch-geometric/torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl\n!pip install /kaggle/input/torch-geometric/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n!pip install torch-geometric","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:19:57.013704Z","iopub.execute_input":"2023-12-13T23:19:57.014082Z","iopub.status.idle":"2023-12-13T23:20:32.591281Z","shell.execute_reply.started":"2023-12-13T23:19:57.014053Z","shell.execute_reply":"2023-12-13T23:20:32.590136Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports\nImport all the modules, we will need later.","metadata":{}},{"cell_type":"code","source":"import os\nimport copy\nimport torch\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom torch_geometric.utils import to_networkx\nfrom torch_geometric.data import Data, DataLoader\n\nimport torch.nn.functional as F\nfrom torch.nn import Linear, BatchNorm1d\nfrom torch_geometric.nn import GATv2Conv\n\nfrom types import SimpleNamespace\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:32.593453Z","iopub.execute_input":"2023-12-13T23:20:32.593779Z","iopub.status.idle":"2023-12-13T23:20:36.774474Z","shell.execute_reply.started":"2023-12-13T23:20:32.593751Z","shell.execute_reply":"2023-12-13T23:20:36.773675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's have a look at the data.","metadata":{}},{"cell_type":"code","source":"df_features = pd.read_csv('/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\ndf_edges = pd.read_csv(\"/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\ndf_classes =  pd.read_csv(\"/kaggle/input/elliptic-data-set/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\ndf_classes['class'] = df_classes['class'].map({'unknown': 2, '1': 1, '2': 0})","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:36.776044Z","iopub.execute_input":"2023-12-13T23:20:36.776688Z","iopub.status.idle":"2023-12-13T23:20:54.121179Z","shell.execute_reply.started":"2023-12-13T23:20:36.776654Z","shell.execute_reply":"2023-12-13T23:20:54.120146Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`df_features` contains each node id and corresponding features.","metadata":{}},{"cell_type":"code","source":"# nodes features\ndf_features.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:54.122487Z","iopub.execute_input":"2023-12-13T23:20:54.122868Z","iopub.status.idle":"2023-12-13T23:20:54.153803Z","shell.execute_reply.started":"2023-12-13T23:20:54.122814Z","shell.execute_reply":"2023-12-13T23:20:54.152863Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`df_edges` contains list of edges.","metadata":{}},{"cell_type":"code","source":"# edges\ndf_edges.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:54.156161Z","iopub.execute_input":"2023-12-13T23:20:54.156437Z","iopub.status.idle":"2023-12-13T23:20:54.163959Z","shell.execute_reply.started":"2023-12-13T23:20:54.156413Z","shell.execute_reply":"2023-12-13T23:20:54.163104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`df_classes` contains each node id and corresponding class, where *0 - licit, 1 - ilicit* and *2 - unknown*.","metadata":{}},{"cell_type":"code","source":"# classes\n# 0 — legitimate\n# 1 — fraud\n# 2 — unknown class\ndf_classes.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:54.165078Z","iopub.execute_input":"2023-12-13T23:20:54.165398Z","iopub.status.idle":"2023-12-13T23:20:54.176731Z","shell.execute_reply.started":"2023-12-13T23:20:54.165362Z","shell.execute_reply":"2023-12-13T23:20:54.175903Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here is the class distribution.","metadata":{}},{"cell_type":"code","source":"df_classes['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:54.177863Z","iopub.execute_input":"2023-12-13T23:20:54.178188Z","iopub.status.idle":"2023-12-13T23:20:54.194266Z","shell.execute_reply.started":"2023-12-13T23:20:54.178157Z","shell.execute_reply":"2023-12-13T23:20:54.193405Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing\nNow, let's convert our data into the `torch_geometric.data.Data` object. First, let's make a dataframe that contains both node features and class.","metadata":{}},{"cell_type":"code","source":"# merging node features DF with classes DF\ndf_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\ndf_merge = df_merge.sort_values(0).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:54.195393Z","iopub.execute_input":"2023-12-13T23:20:54.195711Z","iopub.status.idle":"2023-12-13T23:20:54.617118Z","shell.execute_reply.started":"2023-12-13T23:20:54.19568Z","shell.execute_reply":"2023-12-13T23:20:54.616335Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we will need edges in a [COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). To do that, we map `edges.txId1` and `edges.txId2` to the indices of the corresponding nodes in a `df_merge`. In the end we store edge indices as `torch.Tensor`.","metadata":{}},{"cell_type":"code","source":"# mapping nodes to indices\nnodes = df_merge[0].values\nmap_id = {j:i for i,j in enumerate(nodes)}\n\n# mapping edges to indices\nedges = df_edges.copy()\nedges.txId1 = edges.txId1.map(map_id)\nedges.txId2 = edges.txId2.map(map_id)\nedges = edges.astype(int)\n\nedge_index = np.array(edges.values).T\nedge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n\n# weights for the edges are equal in case of model without attention\nweights = torch.tensor([1] * edge_index.shape[1] , dtype=torch.float32)\n\nprint(\"Total amount of edges in DAG:\", edge_index.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:54.618473Z","iopub.execute_input":"2023-12-13T23:20:54.618878Z","iopub.status.idle":"2023-12-13T23:20:56.688111Z","shell.execute_reply.started":"2023-12-13T23:20:54.618807Z","shell.execute_reply":"2023-12-13T23:20:56.687114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we will need node feature matrix. Here we also map each ID to corresponding index. Then, we will store all the unknown nodes in `unclassified_idx`, that's why we remove class 2, since we no longer need it. In the end, we drop everything but features and store it as `torch.Tensor`.","metadata":{}},{"cell_type":"code","source":"# maping node ids to corresponding indexes\nnode_features = df_merge.drop(['txId'], axis=1).copy()\nnode_features[0] = node_features[0].map(map_id)\n\n# store known and unknown nodes\nclassified_idx = node_features['class'].loc[node_features['class'] != 2].index\nunclassified_idx = node_features['class'].loc[node_features['class'] == 2].index\n\n# replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\nnode_features['class'] = node_features['class'].replace(2, 0)\n\nlabels = node_features['class'].values\n\n# drop indeces, class and temporal axes\nnode_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.float32), dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:56.689417Z","iopub.execute_input":"2023-12-13T23:20:56.689806Z","iopub.status.idle":"2023-12-13T23:20:58.078448Z","shell.execute_reply.started":"2023-12-13T23:20:56.689772Z","shell.execute_reply":"2023-12-13T23:20:58.077317Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this cell, we finally create an object of type `torch_geometric.data.Data`. Here, we pass following parameters:\n- `x` – Node feature matrix with shape.\n\n- `edge_index` – Graph connectivity in COO format with shape `[2, num_edges]`.\n\n- `edge_attr` – Edge feature matrix with shape `[num_edges, num_edge_features]`.\n\n- `y` – Graph-level or node-level ground-truth labels with arbitrary shape.\n\nWe also print a brief description of our network.","metadata":{}},{"cell_type":"code","source":"# converting data to PyGeometric graph data format\nelliptic_dataset = Data(x = node_features,\n                        edge_index = edge_index,\n                        edge_attr = weights,\n                        y = torch.tensor(labels, dtype=torch.float32))\n\nprint(f'Number of nodes: {elliptic_dataset.num_nodes}')\nprint(f'Number of node features: {elliptic_dataset.num_features}')\nprint(f'Number of edges: {elliptic_dataset.num_edges}')\nprint(f'Number of edge features: {elliptic_dataset.num_features}')\nprint(f'Average node degree: {elliptic_dataset.num_edges / elliptic_dataset.num_nodes:.2f}')\nprint(f'Number of classes: {len(np.unique(elliptic_dataset.y))}')\nprint(f'Has isolated nodes: {elliptic_dataset.has_isolated_nodes()}')\nprint(f'Has self loops: {elliptic_dataset.has_self_loops()}')\nprint(f'Is directed: {elliptic_dataset.is_directed()}')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:58.079729Z","iopub.execute_input":"2023-12-13T23:20:58.080039Z","iopub.status.idle":"2023-12-13T23:20:58.251026Z","shell.execute_reply.started":"2023-12-13T23:20:58.080013Z","shell.execute_reply":"2023-12-13T23:20:58.250021Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we create a config to keep all parameters in one place.","metadata":{}},{"cell_type":"code","source":"config = SimpleNamespace(seed = 0,\n                         learning_rate = 0.001,\n                         weight_decay = 1e-5,\n                         input_dim = 165,\n                         output_dim = 1,\n                         hidden_size = 128,\n                         num_epochs = 400,\n                         checkpoints_dir = './models/elliptic_gnn',\n                         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n\nprint(\"Using device:\", config.device)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:58.252099Z","iopub.execute_input":"2023-12-13T23:20:58.252405Z","iopub.status.idle":"2023-12-13T23:20:58.325949Z","shell.execute_reply.started":"2023-12-13T23:20:58.25238Z","shell.execute_reply":"2023-12-13T23:20:58.324893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, we split our data into train, valid and test; where train and valid are 0.85 and 0.15 partitions of the classified nodes and test is all unclassified nodes.","metadata":{}},{"cell_type":"code","source":"y_train = labels[classified_idx]\n\n# spliting train set and validation set\n_, _, _, _, train_idx, valid_idx = train_test_split(node_features[classified_idx],\n                                                    y_train,\n                                                    classified_idx,\n                                                    test_size=0.15,\n                                                    random_state=config.seed,\n                                                    stratify=y_train)\n\nelliptic_dataset.train_idx = torch.tensor(train_idx, dtype=torch.long)\nelliptic_dataset.val_idx = torch.tensor(valid_idx, dtype=torch.long)\nelliptic_dataset.test_idx = torch.tensor(unclassified_idx, dtype=torch.long)\n\nprint(\"Train dataset size:\", elliptic_dataset.train_idx.shape[0])\nprint(\"Validation dataset size:\", elliptic_dataset.val_idx.shape[0])\nprint(\"Test dataset size:\", elliptic_dataset.test_idx.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:58.327444Z","iopub.execute_input":"2023-12-13T23:20:58.327764Z","iopub.status.idle":"2023-12-13T23:20:58.54146Z","shell.execute_reply.started":"2023-12-13T23:20:58.327736Z","shell.execute_reply":"2023-12-13T23:20:58.540598Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model and metric definition\nHere, we will use 2 layers of graph attention: `self.gat1` and `self.gat2`, which are initialized with `torch_geometric.nn.GATv2Conv` class. Both attention layers use multi-headed attention, but on the output layer we use averaging, insted of concatenation, as proposed in theory part. We also apply **dropout** and **batch normalization** on each layer to improve stability.\n\nWe also define **accuracy** metric here.","metadata":{}},{"cell_type":"code","source":"class GAT(torch.nn.Module):\n    \"\"\"Graph Attention Network\"\"\"\n    def __init__(self, dim_in, dim_h, dim_out, heads=8):\n        super(GAT, self).__init__()\n        self.norm1 = BatchNorm1d(dim_in)\n        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads,\n                              dropout=0.3)\n        self.norm2 = BatchNorm1d(dim_h*heads)\n        self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=heads,\n                              concat=False, dropout=0.6)\n\n    def forward(self, x, edge_index):\n        h = self.norm1(x)\n        h = self.gat1(h, edge_index)\n        h = self.norm2(h)\n        h = F.leaky_relu(h)\n        out = self.gat2(h, edge_index)\n        return out\n\ndef accuracy(y_pred, y_test, prediction_threshold=0.5):\n    y_pred_label = (torch.sigmoid(y_pred) > prediction_threshold).float()\n\n    correct_results_sum = (y_pred_label == y_test).sum().float()\n    acc = correct_results_sum/y_test.shape[0]\n\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:58.545325Z","iopub.execute_input":"2023-12-13T23:20:58.545612Z","iopub.status.idle":"2023-12-13T23:20:58.55434Z","shell.execute_reply.started":"2023-12-13T23:20:58.545589Z","shell.execute_reply":"2023-12-13T23:20:58.553286Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training\nHere we do simple training and testing for binary classification just like in pure **PyTorch**.\n\n*See [PyTorch Tutorials](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html) for reference.*","metadata":{}},{"cell_type":"code","source":"def train_evaluate(model, data, criterion, optimizer, *args):\n    num_epochs = args[0]\n    checkpoints_dir = args[1]\n    model_filename = args[2]\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    best_loss = float('inf')\n\n    if not os.path.exists(checkpoints_dir):\n        os.makedirs(checkpoints_dir)\n\n    model.train()\n    for epoch in range(num_epochs+1):\n        # Training\n        optimizer.zero_grad()\n        out = model(data.x, data.edge_index)\n        loss = criterion(out[data.train_idx], data.y[data.train_idx].unsqueeze(1))\n        acc = accuracy(out[data.train_idx], data.y[data.train_idx].unsqueeze(1), prediction_threshold=0.5)\n        loss.backward()\n        optimizer.step()\n\n        # Validation\n        val_loss = criterion(out[data.val_idx], data.y[data.val_idx].unsqueeze(1))\n        val_acc = accuracy(out[data.val_idx], data.y[data.val_idx].unsqueeze(1), prediction_threshold=0.5)\n\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n                  f'{acc*100:>6.2f}% | Val Loss: {val_loss:.4f} | '\n                  f'Val Acc: {val_acc*100:.2f}%')\n\n            if val_loss < best_loss:\n                best_loss = val_loss\n                print(\"Saving model for best loss\")\n                checkpoint = {'state_dict': best_model_wts}\n                torch.save(checkpoint, os.path.join(checkpoints_dir, model_filename))\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n    return model\n\ndef test(model, data):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    preds = ((torch.sigmoid(out) > 0.5).float()*1).squeeze(1)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:58.555644Z","iopub.execute_input":"2023-12-13T23:20:58.556001Z","iopub.status.idle":"2023-12-13T23:20:58.569386Z","shell.execute_reply.started":"2023-12-13T23:20:58.555969Z","shell.execute_reply":"2023-12-13T23:20:58.568508Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(config.seed)\n\ngat_model = GAT(config.input_dim, config.hidden_size, config.output_dim).to(config.device)\ndata_train = elliptic_dataset.to(config.device)\n\noptimizer = torch.optim.Adam(gat_model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\ncriterion = torch.nn.BCEWithLogitsLoss()\n\ntrain_evaluate(gat_model,\n               data_train,\n               criterion,\n               optimizer,\n               config.num_epochs,\n               config.checkpoints_dir,\n               'gat_best_model.pth.tar')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:20:58.570638Z","iopub.execute_input":"2023-12-13T23:20:58.570955Z","iopub.status.idle":"2023-12-13T23:25:07.63922Z","shell.execute_reply.started":"2023-12-13T23:20:58.570932Z","shell.execute_reply":"2023-12-13T23:25:07.638237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluating\nFinally, we can evaluate our model. Our main target is to detect fraudsters, which means that we are mostly interested in **illicit F1 score = 0.89**, which is pretty could for our task.\n\nAlso, confusion matrix and full classification report are provided here.","metadata":{}},{"cell_type":"code","source":"gat_model.load_state_dict(torch.load(os.path.join(config.checkpoints_dir, 'gat_best_model.pth.tar'))['state_dict'])\n\ny_test_preds = test(gat_model, data_train)\n\n# confusion matrix on validation data\nconf_mat = confusion_matrix(data_train.y[data_train.val_idx].detach().cpu().numpy(), y_test_preds[valid_idx].cpu())\n\nplt.subplots(figsize=(6,6))\nsns.set(font_scale=1.4)\nsns.heatmap(conf_mat, annot=True, fmt=\".0f\", annot_kws={\"size\": 16}, cbar=False)\nplt.xlabel('Target (true) Class'); plt.ylabel('Output (predicted) class'); plt.title('Confusion Matrix')\nplt.show();\n\nprint(classification_report(data_train.y[data_train.val_idx].detach().cpu().numpy(),\n                            y_test_preds[valid_idx].cpu(),\n                            target_names=['licit', 'illicit']))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T23:25:07.640474Z","iopub.execute_input":"2023-12-13T23:25:07.640779Z","iopub.status.idle":"2023-12-13T23:25:08.149638Z","shell.execute_reply.started":"2023-12-13T23:25:07.640753Z","shell.execute_reply":"2023-12-13T23:25:08.148629Z"},"trusted":true},"outputs":[],"execution_count":null}]}